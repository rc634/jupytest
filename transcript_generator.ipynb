{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae64b126",
   "metadata": {},
   "source": [
    "## Load needed modules and define path to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348d0886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for speech to text\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for pyannote-audio's diarisation\n",
    "import torch\n",
    "from huggingface_hub import HfApi\n",
    "available_pipelines = [p.modelId for p in HfApi().list_models(filter=\"pyannote-audio-pipeline\")]\n",
    "available_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d34af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9964e63",
   "metadata": {},
   "source": [
    "## List Files in Tarxya Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c1c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls voicetarxya/purchased/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4196229",
   "metadata": {},
   "source": [
    "## Specify the file and desired time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://voicetarxya/purchased/in-443303309465-07449988008-20190624-110909-1561370949.138008.wav test_audio_from_s3.wav\n",
    "#!aws s3 cp s3://voicetarxya/purchased/q-801-1562945353.213997.wav test_audio_from_s3.wav\n",
    "!ls *wav\n",
    "\n",
    "base = \"test_audio_from_s3\" \n",
    "audio = base + \".wav\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_START = 0. #480.\n",
    "GLOBAL_END = 60. #540."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60187b7",
   "metadata": {},
   "source": [
    "## Speech to text on specified time segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfa2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sr.Recognizer()\n",
    "audio_sr = sr.AudioFile(audio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d2567",
   "metadata": {},
   "outputs": [],
   "source": [
    "with audio_sr as source:\n",
    "    audiodata = r.record(source, offset=GLOBAL_START, duration = GLOBAL_END-GLOBAL_START)\n",
    "try:\n",
    "    print(r.recognize_google(audiodata,language=\"en-GB\"))\n",
    "except Exception as e:\n",
    "    print(\"Error : \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbafbb5",
   "metadata": {},
   "source": [
    "# DONT RUN THIS NEXT CELL!\n",
    "## this performs the diarisation on the entire audio file, it takes a while to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d49eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dia = pipeline(audio)\n",
    "dia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37efc71",
   "metadata": {},
   "source": [
    "## Print out resulting time buckets of diarisation within desired time section\n",
    "## Visualise the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc56ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for turn, _, speaker in dia.itertracks(yield_label=True):\n",
    "    if (turn.start > GLOBAL_START and turn.start < GLOBAL_END):\n",
    "        print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e844136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we visualize [0, 30] time range\n",
    "from pyannote.core import notebook, Segment\n",
    "notebook.crop = Segment(GLOBAL_START, GLOBAL_END)\n",
    "dia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c132e651",
   "metadata": {},
   "source": [
    "## Speech recognition on the above diarisation time bins and output with labelled speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2bfe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "r = sr.Recognizer()\n",
    "audio_sr = sr.AudioFile(audio)\n",
    "!ls *wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for turn, _, speaker in dia.itertracks(yield_label=True):\n",
    "    start_time = turn.start\n",
    "    end_time = turn.end\n",
    "    if (turn.start > GLOBAL_START and turn.start < GLOBAL_END):\n",
    "        with audio_sr as source:\n",
    "            audiodata = r.record(source, offset=start_time-0.1, duration = end_time-start_time+0.1)\n",
    "        try:\n",
    "            words = r.recognize_google(audiodata,language=\"en-GB\")\n",
    "            #print(f\"*------------------------START---------t={turn.start:.1f}s--------*\")\n",
    "            print(f\"{speaker} : {words}\")\n",
    "            print(\"\")\n",
    "            #print(f\"*-------------------------END----------t={turn.end:.1f}s--------*\")\n",
    "        except Exception as e:\n",
    "            print(f\"{speaker} : ???\")\n",
    "            print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c72290",
   "metadata": {},
   "source": [
    "##  Speech to text on specified time segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f336585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with audio_sr as source:\n",
    "    audiodata = r.record(source, offset=GLOBAL_START, duration = GLOBAL_END-GLOBAL_START)\n",
    "try:\n",
    "    print(r.recognize_google(audiodata,language=\"en-GB\"))\n",
    "except Exception as e:\n",
    "    print(\"Error : \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03f74b7",
   "metadata": {},
   "source": [
    "## My attempt at stitching audio pieces together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1968b2f1",
   "metadata": {},
   "source": [
    "##### idea here is that sometimes continuous speach by one speaker is broken into multiple chunks here, one long chunk is better for speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488f3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_fragments = []\n",
    "for turn, track, speaker in dia.itertracks(yield_label=True):\n",
    "    if (turn.end > GLOBAL_START and turn.start < GLOBAL_END):\n",
    "        speech_fragments += [[speaker,turn.start,turn.end,\"unique\"]]\n",
    "        print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")\n",
    "#for i in range(len(speech_fragments)): print(speech_fragments[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68442360",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in reversed(range(len(speech_fragments)-1)): # the reverse is important as we want to pull the latest finish time to the first start time\n",
    "    frag = speech_fragments[i]\n",
    "    nextfrag = speech_fragments[i+1]\n",
    "    if frag[0]==nextfrag[0]:\n",
    "        nextfrag[1] = frag[1]\n",
    "        frag[2]=nextfrag[2]\n",
    "        nextfrag[3] = \"repeat\"\n",
    "    \n",
    "for frag in speech_fragments: print(frag)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba396e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_speech = []      \n",
    "for frag in speech_fragments:\n",
    "    if (frag[3]==\"unique\"):\n",
    "        fixed_speech += [frag]\n",
    "        \n",
    "for frag in fixed_speech: print(f\"start={frag[1]:.1f}s stop={frag[2]:.1f}s speaker_{frag[0]}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9c409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for frag in fixed_speech:\n",
    "    with audio_sr as source:\n",
    "            audiodata = r.record(source, offset=frag[1]-0.1, duration = frag[2]-frag[1]+0.1)\n",
    "    try:\n",
    "        words = r.recognize_google(audiodata,language=\"en-GB\")\n",
    "        print(f\"*------------------------START---------t={frag[1]:.1f}s--------*\")\n",
    "        print(f\"{frag[0]} : {words}\")\n",
    "        print(f\"*-------------------------END----------t={frag[2]:.1f}s--------*\")\n",
    "    except Exception as e:\n",
    "        print(f\"*------------------------START---------t={frag[1]:.1f}s--------*\")\n",
    "        print(f\"??? \")\n",
    "        print(f\"*-------------------------END----------t={frag[2]:.1f}s--------*\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd7bc7",
   "metadata": {},
   "source": [
    "## hand transcribed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed525fd6",
   "metadata": {},
   "source": [
    "SPEAKER_00 : yeah \n",
    "\n",
    "SPEAKER_01 : but none of it comes back out to me it's a limited company and it just says there the third one is umm there are four four of us it's a mental health trust \n",
    "\n",
    "SPEAKER_00 : yeah \n",
    "\n",
    "SPEAKER_01 : and the money just goes in to pay the employees salary I don't take anything from it \n",
    "\n",
    "SPEAKER_00 : errrr\n",
    "\n",
    "SPEAKER_01 : and nor could I \n",
    "\n",
    "SPEAKER_00 : no no sure erm ok \n",
    "\n",
    "SPEAKER_01 : it's that I don't want to take it and then gum(?) someone goes ah well that wasn't our\n",
    "\n",
    "SPEAKER_00 : no \n",
    "\n",
    "??? \n",
    "\n",
    "SPEAKER_00 : no no no what I'm going to do is I'll based on what you said is i'm going to ring up april(?) UK and clarify that with them cos I think it's important to so\n",
    "\n",
    "SPEAKER_01 : yea\n",
    "\n",
    "SPEAKER_00 : first one is it's open but there's nothing going in and \n",
    "\n",
    "SPEAKER_01 : or out it's not trading \n",
    "\n",
    "SPEAKER_00 : or out not trading ok the second one is cash in but it goes automatically so that's holiday lettings and it automatically just \n",
    "\n",
    "SPEAKER_01 : well yea it sits in the business it sits in the business it's a limited company"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37827b21",
   "metadata": {},
   "source": [
    "## plain speech to text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382b9df",
   "metadata": {},
   "source": [
    "cheers yeah none of it comes back out to me it's a limited company and it just says the third one is there are four of us it's a mental health trust yeah and the money just goes into pay the employees salary I don't take anything from it no could I know no sure ok so that I don't want to take it and then got someone goes all that wasn't are no woman no no no what I'm going to do is I'll basically what you said he's going to ring up 84 UK and clarify that with them so I think it's important to so first one is it's open but there's nothing going in and it's not trading or non-trading ok the second one is cash in but it goes automatically so that's holiday lettings and it automatically just get it it's in the business it's a limited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5691f176",
   "metadata": {},
   "source": [
    "## automatic transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b8919",
   "metadata": {},
   "outputs": [],
   "source": [
    "for frag in fixed_speech:\n",
    "    with audio_sr as source:\n",
    "            audiodata = r.record(source, offset=frag[1]-0.1, duration = frag[2]-frag[1]+0.1)\n",
    "    try:\n",
    "        words = r.recognize_google(audiodata,language=\"en-GB\")\n",
    "        print(f\"{frag[0]} : {words}\")\n",
    "        print(\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"{frag[0]} : ??? \")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c572f87",
   "metadata": {},
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a3b870",
   "metadata": {},
   "source": [
    "#### -> try ignoring bins of too small width or merge them with bigger bins\n",
    "#### -> try messing with yield_label=True in the dia.intertracks\n",
    "#### -> look into the _ argument too?\n",
    "#### -> try extending bins to regions of no speech, maybe improves accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b5592c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8946305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff842db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
