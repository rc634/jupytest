{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae64b126",
   "metadata": {},
   "source": [
    "## Load needed modules and define path to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b50e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "on_aws = False\n",
    "import time # only needed to time differnet algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f17384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for editing wav files to text\n",
    "from pydub import AudioSegment \n",
    "from pydub.playback import play\n",
    "from pydub.utils import mediainfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "348d0886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for speech to text\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5edb008f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pyannote/speaker-segmentation',\n",
       " 'pyannote/speaker-diarization',\n",
       " 'pyannote/voice-activity-detection',\n",
       " 'pyannote/overlapped-speech-detection']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for pyannote-audio's diarisation\n",
    "import torch\n",
    "from huggingface_hub import HfApi\n",
    "available_pipelines = [p.modelId for p in HfApi().list_models(filter=\"pyannote-audio-pipeline\")]\n",
    "available_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d34af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c1c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list files in tarxya bucket\n",
    "if on_aws:\n",
    "    !aws s3 ls voicetarxya/purchased/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3460b493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT ON AWS!\n",
      "ls: cannot access '*wav': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# specify the audio file and the desired window\n",
    "if on_aws:\n",
    "    print(\"ON AWS!\")\n",
    "    !aws s3 cp s3://voicetarxya/purchased/in-443303309465-07449988008-20190624-110909-1561370949.138008.wav test_audio_from_s3.wav\n",
    "    #!aws s3 cp s3://voicetarxya/purchased/q-801-1562945353.213997.wav test_audio_from_s3.wav\n",
    "    !ls *wav\n",
    "    base = \"test_audio_from_s3\" \n",
    "    audio = base + \".wav\" \n",
    "    GLOBAL_START = 0. \n",
    "    GLOBAL_END = 60. \n",
    "if not on_aws:\n",
    "    print(\"NOT ON AWS!\")\n",
    "    !ls *wav\n",
    "    base = \"audio\" \n",
    "    audio = base + \".wav\"\n",
    "    GLOBAL_START = 0.\n",
    "    GLOBAL_END = 632."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40fbc7d",
   "metadata": {},
   "source": [
    "## use pydub to edit the audio file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7176ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_file = AudioSegment.from_file(file = \"audio.wav\", format = \"wav\")\n",
    "info = mediainfo(\"audio.wav\")\n",
    "ratio = float(len(wav_file))/float(info[\"duration_ts\"])\n",
    "sample_rate = wav_file.frame_rate*ratio\n",
    "start_index = int(GLOBAL_START*sample_rate)\n",
    "end_index = int(GLOBAL_END*sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8798bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_wav_file = wav_file[start_index:end_index]\n",
    "modified_wav_file.export(out_f = \"segment.wav\" , format = \"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dadd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio file\n",
    "modified_wav_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60187b7",
   "metadata": {},
   "source": [
    "## Speech to text on specified time segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfa2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sr.Recognizer()\n",
    "audio_sr = sr.AudioFile(\"segment.wav\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d2567",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "with audio_sr as source:\n",
    "    audiodata = r.record(source)#, offset=GLOBAL_START, duration = GLOBAL_END-GLOBAL_START)\n",
    "try:\n",
    "    print(r.recognize_google(audiodata,language=\"en-GB\"))\n",
    "except Exception as e:\n",
    "    print(\"Error : \" + str(e))\n",
    "end_time = time.time()\n",
    "print(\"\\nTime taken : \",end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbafbb5",
   "metadata": {},
   "source": [
    "# WARNING THE NEXT CELL IS SLOW TO RUN, DONT RUN UNLESS NEED!\n",
    "## this performs the diarisation on the entire audio file, it takes a while to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d49eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diarization\n",
    "start_time = time.time()\n",
    "dia = pipeline(\"segment.wav\")\n",
    "end_time = time.time()\n",
    "end_time = time.time()\n",
    "print(\"\\nTime taken : \",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682f11b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot whole dia\n",
    "dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866907cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can be used to crop the diarization if you dont do that before\n",
    "# crop time interval and replot\n",
    "#from pyannote.core import notebook, Segment\n",
    "#notebook.crop = Segment(GLOBAL_START, GLOBAL_END)\n",
    "#dia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37efc71",
   "metadata": {},
   "source": [
    "## Print out resulting time buckets of diarisation within desired time section\n",
    "## Visualise the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc56ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for turn, _, speaker in dia.itertracks(yield_label=True):\n",
    "    if (True):#turn.start > GLOBAL_START and turn.start < GLOBAL_END):\n",
    "        print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c132e651",
   "metadata": {},
   "source": [
    "## Speech recognition on the above diarisation time bins and output with labelled speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2bfe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sr.Recognizer()\n",
    "audio_sr = sr.AudioFile(\"segment.wav\")\n",
    "!ls *wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for turn, _, speaker in dia.itertracks(yield_label=True):\n",
    "    start_time = turn.start\n",
    "    end_time = turn.end\n",
    "    if (True):#turn.start > GLOBAL_START and turn.start < GLOBAL_END):\n",
    "        with audio_sr as source:\n",
    "            audiodata = r.record(source, offset=start_time-0.1, duration = end_time-start_time+0.1)\n",
    "        try:\n",
    "            words = r.recognize_google(audiodata,language=\"en-GB\")\n",
    "            print(f\"*------------------------START---------t={turn.start:.1f}s--------*\")\n",
    "            print(f\"{speaker} : {words}\")\n",
    "            print(\"\")\n",
    "            print(f\"*-------------------------END----------t={turn.end:.1f}s--------*\")\n",
    "        except Exception as e:\n",
    "            print(f\"{speaker} : ???\")\n",
    "            print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03f74b7",
   "metadata": {},
   "source": [
    "## My attempt at stitching audio pieces together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1968b2f1",
   "metadata": {},
   "source": [
    "##### idea here is that sometimes continuous speach by one speaker is broken into multiple chunks here, one long chunk is better for speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488f3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list each bin output from diariazation, some adjacent bins have the same speaker\n",
    "\n",
    "speech_fragments = []\n",
    "for turn, track, speaker in dia.itertracks(yield_label=True):\n",
    "    if (True):#turn.end > GLOBAL_START and turn.start < GLOBAL_END):\n",
    "        speech_fragments += [[speaker,turn.start,turn.end,\"new\"]]\n",
    "        print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")\n",
    "#for i in range(len(speech_fragments)): print(speech_fragments[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68442360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label if speaker is new or repeated and \n",
    "# make the \"new\" buckets have start and end times that absorb the later buckets with same speaker\n",
    "\n",
    "# the reverse loop is important as we want to pull the latest finish time to the first start time\n",
    "\n",
    "for i in reversed(range(len(speech_fragments)-1)): \n",
    "    frag = speech_fragments[i]\n",
    "    nextfrag = speech_fragments[i+1]\n",
    "    if frag[0]==nextfrag[0]:\n",
    "        nextfrag[1] = frag[1]\n",
    "        frag[2]=nextfrag[2]\n",
    "        nextfrag[3] = \"repeat\"\n",
    "    \n",
    "for frag in speech_fragments: print(frag)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba396e3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# output the modified/grrouped buckets so that there are no more neighbouting buckets with same speaker\n",
    "\n",
    "fixed_speech = []      \n",
    "for frag in speech_fragments:\n",
    "    if (frag[3]==\"new\"):\n",
    "        fixed_speech += [frag]\n",
    "        \n",
    "for frag in fixed_speech: print(f\"start={frag[1]:.1f}s stop={frag[2]:.1f}s speaker_{frag[0]}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01843f2f",
   "metadata": {},
   "source": [
    "## Here print the STT on the grouped speech buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9c409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for frag in fixed_speech:\n",
    "    with audio_sr as source:\n",
    "            audiodata = r.record(source, offset=frag[1]-0.1, duration = frag[2]-frag[1]+0.1)\n",
    "    try:\n",
    "        words = r.recognize_google(audiodata,language=\"en-GB\")\n",
    "        #print(f\"*------------------------START---------t={frag[1]:.1f}s--------*\")\n",
    "        print(f\"{frag[0]} : {words}\")\n",
    "        print(\"\")\n",
    "        #print(f\"*-------------------------END----------t={frag[2]:.1f}s--------*\")\n",
    "    except Exception as e:\n",
    "        #print(f\"*------------------------START---------t={frag[1]:.1f}s--------*\")\n",
    "        print(f\"{frag[0]} : ??? \")\n",
    "        print(\"\")\n",
    "        #print(f\"*-------------------------END----------t={frag[2]:.1f}s--------*\")\n",
    "end_time = time.time()\n",
    "print(f\"Transcript time : {end_time-start_time}s\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd7bc7",
   "metadata": {},
   "source": [
    "## hand transcribed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed525fd6",
   "metadata": {},
   "source": [
    "SPEAKER_00 : yeah \n",
    "\n",
    "SPEAKER_01 : but none of it comes back out to me it's a limited company and it just says there the third one is umm there are four four of us it's a mental health trust \n",
    "\n",
    "SPEAKER_00 : yeah \n",
    "\n",
    "SPEAKER_01 : and the money just goes in to pay the employees salary I don't take anything from it \n",
    "\n",
    "SPEAKER_00 : errrr\n",
    "\n",
    "SPEAKER_01 : and nor could I \n",
    "\n",
    "SPEAKER_00 : no no sure erm ok \n",
    "\n",
    "SPEAKER_01 : it's that I don't want to take it and then gum(?) someone goes ah well that wasn't our\n",
    "\n",
    "SPEAKER_00 : no \n",
    "\n",
    "??? \n",
    "\n",
    "SPEAKER_00 : no no no what I'm going to do is I'll based on what you said is i'm going to ring up april(?) UK and clarify that with them cos I think it's important to so\n",
    "\n",
    "SPEAKER_01 : yea\n",
    "\n",
    "SPEAKER_00 : first one is it's open but there's nothing going in and \n",
    "\n",
    "SPEAKER_01 : or out it's not trading \n",
    "\n",
    "SPEAKER_00 : or out not trading ok the second one is cash in but it goes automatically so that's holiday lettings and it automatically just \n",
    "\n",
    "SPEAKER_01 : well yea it sits in the business it sits in the business it's a limited company"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c572f87",
   "metadata": {},
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a3b870",
   "metadata": {},
   "source": [
    "#### -> try ignoring bins of too small width or merge them with bigger bins\n",
    "#### -> try messing with yield_label=True in the dia.intertracks\n",
    "#### -> look into the _ argument too?\n",
    "#### -> try extending bins to regions of no speech, maybe improves accuracy?\n",
    "#### -> identify customer vs seller from use of works like I and me vs we or you? other indicators\n",
    "#### -> write a wrappper function around the deepspeech as its a bit cumbersome.\n",
    "#### -> maybe think about binning start and end buckets if they are too small, or finding the start/finish of buckets of they are slightly outside the diarization - might need to diarize t_0<->t_1 and t_0-delta <-> t_1+delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c015f",
   "metadata": {},
   "source": [
    "## wordcounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8946305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "start_time = time.time()\n",
    "customer_counts = defaultdict(int)\n",
    "seller_counts = defaultdict(int)\n",
    "diff_counts = defaultdict(int)\n",
    "\n",
    "for frag in fixed_speech:\n",
    "    with audio_sr as source:\n",
    "        audiodata = r.record(source, offset=frag[1]-0.1, duration = frag[2]-frag[1]+0.1)\n",
    "    try:\n",
    "        words = r.recognize_google(audiodata,language=\"en-GB\")\n",
    "        if frag[0] == 'SPEAKER_00':\n",
    "            for word in re.findall('\\w+', words.replace(\"'\",\" \")):\n",
    "                diff_counts[word] -= 1\n",
    "                customer_counts[word] += 1\n",
    "        elif frag[0] == 'SPEAKER_01':\n",
    "            for word in re.findall('\\w+', words.replace(\"'\",\" \")):\n",
    "                diff_counts[word] += 1\n",
    "                seller_counts[word] += 1\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Wordcount time : {end_time-start_time}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a40f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_list = sorted(diff_counts.items(), key=lambda x:x[1])\n",
    "sorted_diff_counts = dict(marked_list)\n",
    "sorted_diff_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191c73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_list = sorted(customer_counts.items(), key=lambda x:x[1])\n",
    "customer_counts = dict(marked_list)\n",
    "customer_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4423b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_list = sorted(seller_counts.items(), key=lambda x:x[1])\n",
    "seller_counts = dict(marked_list)\n",
    "seller_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff842db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_thing = \"hello hello here is a listy poo of words yes yes hello\"\n",
    "splitted = string_thing.split()\n",
    "print(string_thing)\n",
    "splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808637ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_time = 0.\n",
    "seller_time = 0.\n",
    "\n",
    "for frag in fixed_speech:\n",
    "    if frag[0] == 'SPEAKER_01':\n",
    "        customer_time += frag[2]-frag[1]\n",
    "    elif frag[0] == 'SPEAKER_00':\n",
    "        seller_time += frag[2]-frag[1]\n",
    "print(f\"Sellers talking time : {round(seller_time)}s\\nCustomers talking time : {round(customer_time)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa1b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0127d99b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f649806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb5835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b17ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb69530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfeee45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
